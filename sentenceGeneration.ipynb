{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn                                            \n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, bert_encoder, hidden_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bert = bert_encoder\n",
    "        self.hidden2mean = nn.Linear(hidden_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        mean = self.hidden2mean(hidden_state)\n",
    "        logvar = self.hidden2logvar(hidden_state)\n",
    "        return mean, logvar\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(z_dim, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)  \n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, z, target_ids=None, teacher_forcing_ratio=0.5):\n",
    "        h = torch.tanh(self.fc(z)).unsqueeze(0)  \n",
    "        batch_size = z.size(0)\n",
    "        max_length = target_ids.size(1) if target_ids is not None else 20  \n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_length, self.output_layer.out_features).to(z.device)\n",
    "        \n",
    "        input_token = torch.zeros(batch_size, 1, hidden_dim).to(z.device)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            output, h = self.gru(input_token, h)\n",
    "            output_logits = self.output_layer(output.squeeze(1))\n",
    "            outputs[:, t, :] = output_logits\n",
    "            \n",
    "            if target_ids is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                input_token = self.embedding(target_ids[:, t]).unsqueeze(1)  \n",
    "            else:\n",
    "                _, top_token = output_logits.max(dim=1)\n",
    "                input_token = self.embedding(top_token).unsqueeze(1)  \n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SentenceVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, z_dim):\n",
    "        super(SentenceVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def sample_z(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)  \n",
    "        epsilon = torch.randn_like(std)  \n",
    "        z = mean + std * epsilon  \n",
    "        return z\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, target_ids=None, teacher_forcing_ratio=1.0):\n",
    "        mean, logvar = self.encoder(input_ids, attention_mask)\n",
    "        z = self.sample_z(mean, logvar) \n",
    "        recon_x = self.decoder(z, target_ids=target_ids, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        return recon_x, mean, logvar\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0):\n",
    "    logits = logits / temperature  \n",
    "    probabilities = torch.softmax(logits, dim=-1) \n",
    "    return torch.multinomial(probabilities, 1).squeeze(-1)  \n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "hidden_dim = 768\n",
    "z_dim = 16  \n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "encoder = Encoder(distilbert_encoder, hidden_dim, z_dim)\n",
    "decoder = Decoder(z_dim, hidden_dim, vocab_size)\n",
    "model = SentenceVAE(encoder, decoder, z_dim=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variation 1: i have made the story is all the most comedy, this movie and it's a to make one movie of her.\n",
      "Variation 2: despite its own, of the.s the's worth all the first ( and the'll be an intelligent, but i can be even for the it, and an, and is the only to look.\n",
      "Variation 3: it's the film that is so much a very documentary inventive..\n",
      "Variation 4: a one of the film - -, it's a a lot of the. or a story's life of the worst.\n"
     ]
    }
   ],
   "source": [
    "def load_state_dict(model, filepath):\n",
    "    state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"module.\", \"\") if key.startswith(\"module.\") else key\n",
    "        new_state_dict[new_key] = value\n",
    "    model.load_state_dict(new_state_dict)\n",
    " \n",
    "load_state_dict(model, \"best_sentence_model.pt\")\n",
    "model.eval()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def generate_variations(model, input_text, max_length=256, temperature=0.7, top_k=50, num_variations=4, perturb_scale=0.1):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        add_special_tokens=True, \n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, logvar = model.encoder(input_ids, attention_mask)\n",
    "\n",
    "        variations = []\n",
    "        for _ in range(num_variations):\n",
    "            noise = torch.randn_like(mean) * perturb_scale\n",
    "            z = model.sample_z(mean, logvar) + noise\n",
    "            \n",
    "            generated_ids = [tokenizer.cls_token_id]  \n",
    "            input_token = model.decoder.embedding(torch.tensor([[tokenizer.cls_token_id]]).to(device))  \n",
    "            h = torch.tanh(model.decoder.fc(z)).unsqueeze(0)  \n",
    "            for _ in range(max_length):\n",
    "                output, h = model.decoder.gru(input_token, h)\n",
    "                logits = model.decoder.output_layer(output.squeeze(1)) / temperature \n",
    "\n",
    "                k = min(top_k, logits.size(-1)) \n",
    "                top_k_values, top_k_indices = torch.topk(logits, k)\n",
    "                probabilities = F.softmax(top_k_values, dim=-1)\n",
    "                \n",
    "                next_token_index = torch.multinomial(probabilities, 1).item()  \n",
    "                next_token_id = top_k_indices[0, next_token_index].item()  \n",
    "\n",
    "                word = tokenizer.decode([next_token_id])\n",
    "\n",
    "                generated_ids.append(next_token_id)\n",
    "                if next_token_id == tokenizer.sep_token_id: \n",
    "                    break\n",
    "\n",
    "                input_token = model.decoder.embedding(torch.tensor([[next_token_id]]).to(device))\n",
    "\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            variations.append(generated_text)\n",
    "        \n",
    "    return variations\n",
    "\n",
    "input_text = \"This show is so good, I want to watch it again. I love it! The plot is amazing and the acting is great.\"\n",
    "variations = generate_variations(model, input_text, temperature=0.9, top_k=50, num_variations=4, perturb_scale=0.1)\n",
    "for i, sentence in enumerate(variations, 1):\n",
    "    print(f\"Variation {i}: {sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
