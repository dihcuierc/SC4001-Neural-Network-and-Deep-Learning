{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:42:39.146090Z",
     "iopub.status.busy": "2024-11-14T17:42:39.145620Z",
     "iopub.status.idle": "2024-11-14T17:42:40.826403Z",
     "shell.execute_reply": "2024-11-14T17:42:40.825418Z",
     "shell.execute_reply.started": "2024-11-14T17:42:39.146041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100-sample dataset: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW, GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "seed = 42\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "df_200 = df.sample(n=200, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "df_100 = df_200.iloc[:100].reset_index(drop=True)\n",
    "df_test = df_200.iloc[100:].reset_index(drop=True)\n",
    "print(\"100-sample dataset:\", df_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:42:40.828618Z",
     "iopub.status.busy": "2024-11-14T17:42:40.828282Z",
     "iopub.status.idle": "2024-11-14T17:42:40.896351Z",
     "shell.execute_reply": "2024-11-14T17:42:40.895384Z",
     "shell.execute_reply.started": "2024-11-14T17:42:40.828585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data_df):\n",
    "    data_df['text_length'] = data_df['review'].apply(len)\n",
    "    data_df['target'] = data_df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "    all_words = [word for review in data_df[\"review\"] for word in review.split(\" \")]\n",
    "    vocab = Counter(all_words)\n",
    "    counts = list(vocab.values())\n",
    "    data_df[\"word_count\"] = data_df[\"review\"].apply(lambda x: len(x.split(\" \")))\n",
    "    data_df[\"word_count\"].describe()\n",
    "    data_df[['text_length', 'target']].groupby('target').mean()['text_length']\n",
    "    data_df[data_df[\"target\"] == 0][\"review\"].values[1]\n",
    "    def remove_hashtag(text):\n",
    "        pattern= \"#[\\w\\d]+\"\n",
    "        return re.sub(pattern, \"\", text)\n",
    "\n",
    "    def remove_url(text):\n",
    "        pattern = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "        return re.sub(pattern, \"\", text)\n",
    "        \n",
    "    data_df['text_without_hastag'] = data_df['review'].apply(remove_hashtag)\n",
    "    data_df['text_without_url'] = data_df['text_without_hastag'].apply(remove_url)\n",
    "    pattern = r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "\n",
    "    def count_non_english(text):\n",
    "        return len(re.findall(pattern, text))\n",
    "\n",
    "    data_df['non_english_count'] = data_df['text_without_url'].apply(count_non_english)\n",
    "    def remove_non_english(text):\n",
    "        pattern = r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "        return re.sub(pattern, \"\", text)\n",
    "    data_df['text_without_non_english'] = data_df['text_without_url'].apply(remove_non_english)\n",
    "    def remove_markdown(text):\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text) \n",
    "        text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "    # Apply the function to the 'review' column\n",
    "    data_df['cleaned'] = data_df['text_without_non_english'].apply(remove_markdown)\n",
    "\n",
    "    train_df, val_df= train_test_split(data_df[['cleaned', 'target']], test_size=0.2, random_state=90,shuffle=True, stratify=data_df['target'])\n",
    "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df_100, val_df_100 = clean_data(df_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:42:40.898235Z",
     "iopub.status.busy": "2024-11-14T17:42:40.897840Z",
     "iopub.status.idle": "2024-11-14T17:42:42.173530Z",
     "shell.execute_reply": "2024-11-14T17:42:42.172314Z",
     "shell.execute_reply.started": "2024-11-14T17:42:40.898190Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5727d02307436381c34d1690d49014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187e3a6cfe524594b7d7dbccd52ceae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655e0427294545f898a64735d2fc3136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e898b004a18d4fdaa0f05d25563b215f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceb712339e1404491ef027296badd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Dataset and Tokenize\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token \n",
    "\n",
    "max_length = 256\n",
    "train_dataset_100 = SentimentDataset(train_df_100['cleaned'].to_list(), train_df_100['target'].to_list(), gpt_tokenizer, max_length=max_length)\n",
    "val_dataset_100 = SentimentDataset(val_df_100['cleaned'].to_list(), val_df_100['target'].to_list(), gpt_tokenizer, max_length=max_length)\n",
    "batch_size = 32\n",
    "train_dataloader_100 = DataLoader(train_dataset_100, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_100 = DataLoader(val_dataset_100, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:42:42.176942Z",
     "iopub.status.busy": "2024-11-14T17:42:42.176567Z",
     "iopub.status.idle": "2024-11-14T17:42:42.201565Z",
     "shell.execute_reply": "2024-11-14T17:42:42.200522Z",
     "shell.execute_reply.started": "2024-11-14T17:42:42.176907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, num_epochs=50, patience=5, clip_value=1.0, learning_rate=2e-5, save_name=\"classifier.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Tracking metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect predictions for accuracy calculation\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            train_f1 = f1_score(labels, preds, average='weighted')\n",
    "            all_train_preds.extend(preds)\n",
    "            all_train_labels.extend(labels)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        \n",
    "        train_f1s.append(train_f1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "                val_f1 = f1_score(labels, preds, average='weighted')\n",
    "                all_val_preds.extend(preds)\n",
    "                all_val_labels.extend(labels)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "\n",
    "        val_f1s.append(val_f1)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.3f}, Train Accuracy: {train_accuracy:.3f}, Train F1: {train_f1:.3f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.3f}, Val Accuracy: {val_accuracy:.3f}, Val F1: {val_f1:.3f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), save_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Training complete\n",
    "    print(\"Training complete.\")\n",
    "    print(f\"Average epoch time: {sum(epoch_times) / len(epoch_times):.2f} seconds\")\n",
    "    print(f\"Total training time: {sum(epoch_times):.2f} seconds\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:42:42.203205Z",
     "iopub.status.busy": "2024-11-14T17:42:42.202792Z",
     "iopub.status.idle": "2024-11-14T17:44:02.751714Z",
     "shell.execute_reply": "2024-11-14T17:44:02.750633Z",
     "shell.execute_reply.started": "2024-11-14T17:42:42.203158Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805e5864ba3c4e47ac332e489162f6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Time: 4.92s\n",
      "Train Loss: 4.435, Train Accuracy: 0.487, Train F1: 0.767\n",
      "Val Loss: 4.610, Val Accuracy: 0.500, Val F1: 0.333\n",
      "Epoch 2/50 - Time: 3.93s\n",
      "Train Loss: 4.330, Train Accuracy: 0.487, Train F1: 0.333\n",
      "Val Loss: 3.981, Val Accuracy: 0.500, Val F1: 0.333\n",
      "Epoch 3/50 - Time: 3.99s\n",
      "Train Loss: 3.409, Train Accuracy: 0.463, Train F1: 0.531\n",
      "Val Loss: 3.172, Val Accuracy: 0.450, Val F1: 0.310\n",
      "Epoch 4/50 - Time: 3.94s\n",
      "Train Loss: 2.735, Train Accuracy: 0.487, Train F1: 0.266\n",
      "Val Loss: 2.390, Val Accuracy: 0.500, Val F1: 0.405\n",
      "Epoch 5/50 - Time: 4.05s\n",
      "Train Loss: 1.704, Train Accuracy: 0.525, Train F1: 0.377\n",
      "Val Loss: 2.204, Val Accuracy: 0.500, Val F1: 0.495\n",
      "Epoch 6/50 - Time: 4.04s\n",
      "Train Loss: 1.185, Train Accuracy: 0.588, Train F1: 0.750\n",
      "Val Loss: 2.489, Val Accuracy: 0.400, Val F1: 0.375\n",
      "Epoch 7/50 - Time: 4.00s\n",
      "Train Loss: 1.266, Train Accuracy: 0.575, Train F1: 0.568\n",
      "Val Loss: 2.192, Val Accuracy: 0.400, Val F1: 0.400\n",
      "Epoch 8/50 - Time: 4.01s\n",
      "Train Loss: 0.977, Train Accuracy: 0.625, Train F1: 0.500\n",
      "Val Loss: 1.725, Val Accuracy: 0.350, Val F1: 0.307\n",
      "Epoch 9/50 - Time: 4.12s\n",
      "Train Loss: 0.795, Train Accuracy: 0.725, Train F1: 0.806\n",
      "Val Loss: 1.548, Val Accuracy: 0.350, Val F1: 0.307\n",
      "Epoch 10/50 - Time: 4.12s\n",
      "Train Loss: 0.740, Train Accuracy: 0.750, Train F1: 0.742\n",
      "Val Loss: 1.495, Val Accuracy: 0.350, Val F1: 0.307\n",
      "Epoch 11/50 - Time: 4.06s\n",
      "Train Loss: 0.501, Train Accuracy: 0.850, Train F1: 1.000\n",
      "Val Loss: 1.631, Val Accuracy: 0.450, Val F1: 0.437\n",
      "Epoch 12/50 - Time: 4.11s\n",
      "Train Loss: 0.407, Train Accuracy: 0.825, Train F1: 0.819\n",
      "Val Loss: 1.984, Val Accuracy: 0.500, Val F1: 0.500\n",
      "Epoch 13/50 - Time: 4.10s\n",
      "Train Loss: 0.487, Train Accuracy: 0.812, Train F1: 0.750\n",
      "Val Loss: 1.906, Val Accuracy: 0.500, Val F1: 0.495\n",
      "Epoch 14/50 - Time: 4.11s\n",
      "Train Loss: 0.326, Train Accuracy: 0.863, Train F1: 0.865\n",
      "Val Loss: 1.798, Val Accuracy: 0.500, Val F1: 0.495\n",
      "Epoch 15/50 - Time: 4.18s\n",
      "Train Loss: 0.260, Train Accuracy: 0.900, Train F1: 0.875\n",
      "Val Loss: 1.821, Val Accuracy: 0.500, Val F1: 0.495\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Average epoch time: 4.11 seconds\n",
      "Total training time: 61.68 seconds\n",
      "[4.434674302736918, 4.329610824584961, 3.409327665964762, 2.7350297768910727, 1.7044060230255127, 1.1845124165217082, 1.2657535473505657, 0.977274219195048, 0.7954808473587036, 0.7400700449943542, 0.5009236832459768, 0.4071661134560903, 0.4873315493265788, 0.32582782705624896, 0.2604365348815918]\n",
      "[4.61026668548584, 3.980696201324463, 3.1720380783081055, 2.3903274536132812, 2.2036890983581543, 2.4894537925720215, 2.1918156147003174, 1.7248640060424805, 1.548054814338684, 1.495197057723999, 1.6312106847763062, 1.9842770099639893, 1.9055970907211304, 1.7982654571533203, 1.8208290338516235]\n",
      "[0.4875, 0.4875, 0.4625, 0.4875, 0.525, 0.5875, 0.575, 0.625, 0.725, 0.75, 0.85, 0.825, 0.8125, 0.8625, 0.9]\n",
      "[0.5, 0.5, 0.45, 0.5, 0.5, 0.4, 0.4, 0.35, 0.35, 0.35, 0.45, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "train_results_100 = train_model(train_dataloader_100, val_dataloader_100, save_name=\"decoder_100.pt\")\n",
    "print(train_results_100[\"train_losses\"])\n",
    "print(train_results_100[\"val_losses\"])\n",
    "print(train_results_100[\"train_accuracies\"])\n",
    "print(train_results_100[\"val_accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:44:02.754345Z",
     "iopub.status.busy": "2024-11-14T17:44:02.753682Z",
     "iopub.status.idle": "2024-11-14T17:44:08.915209Z",
     "shell.execute_reply": "2024-11-14T17:44:08.914287Z",
     "shell.execute_reply.started": "2024-11-14T17:44:02.754297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a269d0a02d2435a9ffb0ce68dfdbeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d888e4f7ebc49adaee4cfafd0a99243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97639f6d58994537a65bb3a4158f904e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b183fb60cb6b4a7bb43ce621166e8647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324ffffd99dc466c811687593fa6ef16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1174272457.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (encoder): Encoder(\n",
       "    (bert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (hidden2mean): Linear(in_features=768, out_features=16, bias=True)\n",
       "    (hidden2logvar): Linear(in_features=768, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=16, out_features=768, bias=True)\n",
       "    (embedding): Embedding(30522, 768)\n",
       "    (gru): GRU(768, 768, batch_first=True)\n",
       "    (output_layer): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, bert_encoder, hidden_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bert = bert_encoder\n",
    "        self.hidden2mean = nn.Linear(hidden_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        mean = self.hidden2mean(hidden_state)\n",
    "        logvar = self.hidden2logvar(hidden_state)\n",
    "        return mean, logvar\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(z_dim, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)  # Embedding layer for input tokens\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, z, target_ids=None, teacher_forcing_ratio=0.5):\n",
    "        h = torch.tanh(self.fc(z)).unsqueeze(0)  # Initial hidden state from latent vector z\n",
    "        batch_size = z.size(0)\n",
    "        max_length = target_ids.size(1) if target_ids is not None else 20  # Set max length\n",
    "\n",
    "        # Initialize output tensor to store logits\n",
    "        outputs = torch.zeros(batch_size, max_length, self.output_layer.out_features).to(z.device)\n",
    "        \n",
    "        # Initialize input token (you may replace this with the start token if available)\n",
    "        input_token = torch.zeros(batch_size, 1, hidden_dim).to(z.device)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            output, h = self.gru(input_token, h)\n",
    "            output_logits = self.output_layer(output.squeeze(1))\n",
    "            outputs[:, t, :] = output_logits\n",
    "            \n",
    "            # Teacher forcing: use ground truth with probability teacher_forcing_ratio\n",
    "            if target_ids is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                input_token = self.embedding(target_ids[:, t]).unsqueeze(1)  # Convert token ID to embedding\n",
    "            else:\n",
    "                _, top_token = output_logits.max(dim=1)\n",
    "                input_token = self.embedding(top_token).unsqueeze(1)  # Convert predicted token ID to embedding\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SentenceVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, z_dim):\n",
    "        super(SentenceVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def sample_z(self, mean, logvar):\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        epsilon = torch.randn_like(std)  # Sample noise\n",
    "        z = mean + std * epsilon  # Sample z\n",
    "        return z\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, target_ids=None, teacher_forcing_ratio=1.0):\n",
    "        # Encode to obtain mean and logvar\n",
    "        mean, logvar = self.encoder(input_ids, attention_mask)\n",
    "        z = self.sample_z(mean, logvar)  # Sample latent vector z\n",
    "        # Decode the latent vector z, using target_ids and teacher_forcing_ratio if provided\n",
    "        recon_x = self.decoder(z, target_ids=target_ids, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        return recon_x, mean, logvar\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0):\n",
    "    logits = logits / temperature  # Scale by temperature\n",
    "    probabilities = torch.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "    return torch.multinomial(probabilities, 1).squeeze(-1)  # Sample from the distribution\n",
    "\n",
    "def load_state_dict(model, filepath):\n",
    "    state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"module.\", \"\") if key.startswith(\"module.\") else key\n",
    "        new_state_dict[new_key] = value\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "hidden_dim = 768\n",
    "z_dim = 16  # Latent space dimensionality\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "encoder = Encoder(distilbert_encoder, hidden_dim, z_dim)\n",
    "decoder = Decoder(z_dim, hidden_dim, vocab_size)\n",
    "model = SentenceVAE(encoder, decoder, z_dim=16)  # Adjust based on your model's structure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "load_state_dict(model, \"/kaggle/input/best_sentencevae/pytorch/default/1/best_sentence_model (3).pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:44:08.917028Z",
     "iopub.status.busy": "2024-11-14T17:44:08.916613Z",
     "iopub.status.idle": "2024-11-14T17:44:08.932283Z",
     "shell.execute_reply": "2024-11-14T17:44:08.931350Z",
     "shell.execute_reply.started": "2024-11-14T17:44:08.916984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_variations(model, input_text, max_length=256, temperature=0.7, top_k=50, num_variations=4, perturb_scale=0.1):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, logvar = model.encoder(input_ids, attention_mask)\n",
    "        \n",
    "        variations = []\n",
    "        for _ in range(num_variations):\n",
    "            noise = torch.randn_like(mean) * perturb_scale\n",
    "            z = model.sample_z(mean, logvar) + noise\n",
    "            \n",
    "            generated_ids = [tokenizer.cls_token_id]\n",
    "            input_token = model.decoder.embedding(torch.tensor([[tokenizer.cls_token_id]]).to(device))\n",
    "            h = torch.tanh(model.decoder.fc(z)).unsqueeze(0)\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                output, h = model.decoder.gru(input_token, h)\n",
    "                logits = model.decoder.output_layer(output.squeeze(1)) / temperature\n",
    "\n",
    "                k = min(top_k, logits.size(-1))\n",
    "                top_k_values, top_k_indices = torch.topk(logits, k)\n",
    "                probabilities = F.softmax(top_k_values, dim=-1)\n",
    "                \n",
    "                next_token_index = torch.multinomial(probabilities, 1).item()\n",
    "                next_token_id = top_k_indices[0, next_token_index].item()\n",
    "\n",
    "                generated_ids.append(next_token_id)\n",
    "                if next_token_id == tokenizer.sep_token_id:\n",
    "                    break\n",
    "\n",
    "                input_token = model.decoder.embedding(torch.tensor([[next_token_id]]).to(device))\n",
    "\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            variations.append(generated_text)\n",
    "        \n",
    "    return variations\n",
    "\n",
    "# Dataset augmentation function\n",
    "def augment_dataset(df, num_variations=4, max_length=256, temperature=0.9, top_k=50, perturb_scale=0.1):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = row['review']\n",
    "        label = row['sentiment']\n",
    "\n",
    "        variations = generate_variations(\n",
    "            model, input_text, max_length=max_length,\n",
    "            temperature=temperature, top_k=top_k, \n",
    "            num_variations=num_variations, perturb_scale=perturb_scale\n",
    "        )\n",
    "\n",
    "        augmented_texts.extend(variations)\n",
    "        augmented_labels.extend([label] * num_variations)\n",
    "\n",
    "    augmented_df = pd.DataFrame({'review': augmented_texts, 'sentiment': augmented_labels})\n",
    "    combined_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:44:08.933815Z",
     "iopub.status.busy": "2024-11-14T17:44:08.933519Z",
     "iopub.status.idle": "2024-11-14T17:44:35.845001Z",
     "shell.execute_reply": "2024-11-14T17:44:35.843671Z",
     "shell.execute_reply.started": "2024-11-14T17:44:08.933784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmented_df_400 = augment_dataset(df_100, num_variations=1)\n",
    "augmented_df_1000 = augment_dataset(df_100, num_variations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:44:35.846900Z",
     "iopub.status.busy": "2024-11-14T17:44:35.846304Z",
     "iopub.status.idle": "2024-11-14T17:44:35.951551Z",
     "shell.execute_reply": "2024-11-14T17:44:35.950483Z",
     "shell.execute_reply.started": "2024-11-14T17:44:35.846860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmented_train_df_400, augmented_val_df_400 = clean_data(augmented_df_400)\n",
    "augmented_train_df_1000, augmented_val_df_1000 = clean_data(augmented_df_1000)\n",
    "\n",
    "max_length = 256\n",
    "augmented_train_dataset_400 = SentimentDataset(augmented_train_df_400['cleaned'], augmented_train_df_400['target'], tokenizer)\n",
    "augmented_val_dataset_400 = SentimentDataset(augmented_val_df_400['cleaned'], augmented_val_df_400['target'], tokenizer)\n",
    "augmented_train_dataset_1000 = SentimentDataset(augmented_train_df_1000['cleaned'], augmented_train_df_1000['target'], tokenizer)\n",
    "augmented_val_dataset_1000 = SentimentDataset(augmented_val_df_1000['cleaned'], augmented_val_df_1000['target'], tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "augmented_train_dataloader_400 = DataLoader(augmented_train_dataset_400, batch_size=batch_size)\n",
    "augmented_val_dataloader_400 = DataLoader(augmented_val_dataset_400, batch_size=batch_size)\n",
    "augmented_train_dataloader_1000 = DataLoader(augmented_train_dataset_1000, batch_size=batch_size)\n",
    "augmented_val_dataloader_1000 = DataLoader(augmented_val_dataset_1000, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:44:35.954903Z",
     "iopub.status.busy": "2024-11-14T17:44:35.954536Z",
     "iopub.status.idle": "2024-11-14T17:45:36.042435Z",
     "shell.execute_reply": "2024-11-14T17:45:36.041458Z",
     "shell.execute_reply.started": "2024-11-14T17:44:35.954864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Time: 4.81s\n",
      "Train Loss: 3.664, Train Accuracy: 0.481, Train F1: 0.333\n",
      "Val Loss: 2.923, Val Accuracy: 0.475, Val F1: 0.333\n",
      "Epoch 2/50 - Time: 4.84s\n",
      "Train Loss: 2.194, Train Accuracy: 0.469, Train F1: 0.304\n",
      "Val Loss: 1.095, Val Accuracy: 0.475, Val F1: 0.333\n",
      "Epoch 3/50 - Time: 4.95s\n",
      "Train Loss: 1.117, Train Accuracy: 0.519, Train F1: 0.355\n",
      "Val Loss: 0.704, Val Accuracy: 0.650, Val F1: 0.365\n",
      "Epoch 4/50 - Time: 4.87s\n",
      "Train Loss: 0.773, Train Accuracy: 0.537, Train F1: 0.584\n",
      "Val Loss: 0.832, Val Accuracy: 0.500, Val F1: 0.333\n",
      "Epoch 5/50 - Time: 4.90s\n",
      "Train Loss: 0.731, Train Accuracy: 0.600, Train F1: 0.746\n",
      "Val Loss: 0.709, Val Accuracy: 0.500, Val F1: 0.619\n",
      "Epoch 6/50 - Time: 4.92s\n",
      "Train Loss: 0.725, Train Accuracy: 0.556, Train F1: 0.417\n",
      "Val Loss: 0.666, Val Accuracy: 0.575, Val F1: 0.564\n",
      "Epoch 7/50 - Time: 4.94s\n",
      "Train Loss: 0.645, Train Accuracy: 0.637, Train F1: 0.625\n",
      "Val Loss: 0.691, Val Accuracy: 0.550, Val F1: 0.333\n",
      "Epoch 8/50 - Time: 5.00s\n",
      "Train Loss: 0.642, Train Accuracy: 0.625, Train F1: 0.750\n",
      "Val Loss: 0.707, Val Accuracy: 0.475, Val F1: 0.333\n",
      "Epoch 9/50 - Time: 5.01s\n",
      "Train Loss: 0.610, Train Accuracy: 0.669, Train F1: 0.749\n",
      "Val Loss: 0.683, Val Accuracy: 0.525, Val F1: 0.333\n",
      "Epoch 10/50 - Time: 5.02s\n",
      "Train Loss: 0.555, Train Accuracy: 0.713, Train F1: 0.746\n",
      "Val Loss: 0.671, Val Accuracy: 0.550, Val F1: 0.564\n",
      "Epoch 11/50 - Time: 5.12s\n",
      "Train Loss: 0.566, Train Accuracy: 0.700, Train F1: 0.776\n",
      "Val Loss: 0.681, Val Accuracy: 0.550, Val F1: 0.333\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Average epoch time: 4.94 seconds\n",
      "Total training time: 54.39 seconds\n",
      "[3.6641994953155517, 2.1936508417129517, 1.1167547941207885, 0.7732653498649598, 0.7307862162590026, 0.724849271774292, 0.6448944211006165, 0.6416081666946412, 0.6097109079360962, 0.5553789258003234, 0.5661495089530945]\n",
      "[2.923094868659973, 1.0950571298599243, 0.7035931348800659, 0.8320452272891998, 0.7094617187976837, 0.6658973395824432, 0.690569281578064, 0.707440584897995, 0.6830094456672668, 0.6713656187057495, 0.6807088851928711]\n",
      "[0.48125, 0.46875, 0.51875, 0.5375, 0.6, 0.55625, 0.6375, 0.625, 0.66875, 0.7125, 0.7]\n",
      "[0.475, 0.475, 0.65, 0.5, 0.5, 0.575, 0.55, 0.475, 0.525, 0.55, 0.55]\n"
     ]
    }
   ],
   "source": [
    "augmented_train_results_400 = train_model(augmented_train_dataloader_400, augmented_val_dataloader_400, save_name=\"augmented_400_decoder\")\n",
    "print(augmented_train_results_400[\"train_losses\"])\n",
    "print(augmented_train_results_400[\"val_losses\"])\n",
    "print(augmented_train_results_400[\"train_accuracies\"])\n",
    "print(augmented_train_results_400[\"val_accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:45:36.044054Z",
     "iopub.status.busy": "2024-11-14T17:45:36.043739Z",
     "iopub.status.idle": "2024-11-14T17:47:43.782299Z",
     "shell.execute_reply": "2024-11-14T17:47:43.781268Z",
     "shell.execute_reply.started": "2024-11-14T17:45:36.044020Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Time: 11.48s\n",
      "Train Loss: 0.856, Train Accuracy: 0.517, Train F1: 0.426\n",
      "Val Loss: 0.723, Val Accuracy: 0.550, Val F1: 0.667\n",
      "Epoch 2/50 - Time: 11.49s\n",
      "Train Loss: 0.728, Train Accuracy: 0.485, Train F1: 0.473\n",
      "Val Loss: 0.704, Val Accuracy: 0.470, Val F1: 0.857\n",
      "Epoch 3/50 - Time: 11.30s\n",
      "Train Loss: 0.707, Train Accuracy: 0.530, Train F1: 0.605\n",
      "Val Loss: 0.720, Val Accuracy: 0.470, Val F1: 0.667\n",
      "Epoch 4/50 - Time: 11.14s\n",
      "Train Loss: 0.694, Train Accuracy: 0.547, Train F1: 0.508\n",
      "Val Loss: 0.727, Val Accuracy: 0.500, Val F1: 0.400\n",
      "Epoch 5/50 - Time: 11.15s\n",
      "Train Loss: 0.677, Train Accuracy: 0.580, Train F1: 0.385\n",
      "Val Loss: 0.708, Val Accuracy: 0.530, Val F1: 0.667\n",
      "Epoch 6/50 - Time: 11.20s\n",
      "Train Loss: 0.673, Train Accuracy: 0.595, Train F1: 0.385\n",
      "Val Loss: 0.694, Val Accuracy: 0.450, Val F1: 0.857\n",
      "Epoch 7/50 - Time: 11.22s\n",
      "Train Loss: 0.653, Train Accuracy: 0.632, Train F1: 0.553\n",
      "Val Loss: 0.716, Val Accuracy: 0.430, Val F1: 0.857\n",
      "Epoch 8/50 - Time: 11.32s\n",
      "Train Loss: 0.628, Train Accuracy: 0.670, Train F1: 0.631\n",
      "Val Loss: 0.756, Val Accuracy: 0.430, Val F1: 0.400\n",
      "Epoch 9/50 - Time: 11.33s\n",
      "Train Loss: 0.632, Train Accuracy: 0.647, Train F1: 0.568\n",
      "Val Loss: 0.731, Val Accuracy: 0.460, Val F1: 0.667\n",
      "Epoch 10/50 - Time: 11.30s\n",
      "Train Loss: 0.618, Train Accuracy: 0.645, Train F1: 0.425\n",
      "Val Loss: 0.713, Val Accuracy: 0.420, Val F1: 0.857\n",
      "Epoch 11/50 - Time: 11.22s\n",
      "Train Loss: 0.604, Train Accuracy: 0.695, Train F1: 0.736\n",
      "Val Loss: 0.727, Val Accuracy: 0.430, Val F1: 0.857\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Average epoch time: 11.29 seconds\n",
      "Total training time: 124.15 seconds\n",
      "[0.8556279769310584, 0.728328654399285, 0.7070986674382136, 0.6942124504309434, 0.6773666922862713, 0.6728429473363436, 0.6527628990320059, 0.6276809160525982, 0.6324257575548612, 0.6180697129322932, 0.6038001065070813]\n",
      "[0.7232476621866226, 0.7036160528659821, 0.7195760756731033, 0.7269385904073715, 0.7077450305223465, 0.693559005856514, 0.7160285115242004, 0.7562693655490875, 0.7311017215251923, 0.713442400097847, 0.7270081639289856]\n",
      "[0.5175, 0.485, 0.53, 0.5475, 0.58, 0.595, 0.6325, 0.67, 0.6475, 0.645, 0.695]\n",
      "[0.55, 0.47, 0.47, 0.5, 0.53, 0.45, 0.43, 0.43, 0.46, 0.42, 0.43]\n"
     ]
    }
   ],
   "source": [
    "augmented_train_results_1000 = train_model(augmented_train_dataloader_1000, augmented_val_dataloader_1000, save_name=\"augmented_1000_decoder\")\n",
    "print(augmented_train_results_1000[\"train_losses\"])\n",
    "print(augmented_train_results_1000[\"val_losses\"])\n",
    "print(augmented_train_results_1000[\"train_accuracies\"])\n",
    "print(augmented_train_results_1000[\"val_accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:47:43.784339Z",
     "iopub.status.busy": "2024-11-14T17:47:43.783665Z",
     "iopub.status.idle": "2024-11-14T17:47:43.794765Z",
     "shell.execute_reply": "2024-11-14T17:47:43.793729Z",
     "shell.execute_reply.started": "2024-11-14T17:47:43.784290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_dataset(dataloader, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "    model.config.pad_token_id = model.config.eos_token_id \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Track metrics\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            val_f1 = f1_score(labels, preds, average='weighted')\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "\n",
    "    # Calculate average loss, accuracy, and F1 score\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # Use 'weighted' to account for class imbalance\n",
    "\n",
    "    # Output metrics\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T17:47:43.796238Z",
     "iopub.status.busy": "2024-11-14T17:47:43.795934Z",
     "iopub.status.idle": "2024-11-14T17:47:46.783213Z",
     "shell.execute_reply": "2024-11-14T17:47:46.782263Z",
     "shell.execute_reply.started": "2024-11-14T17:47:43.796205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/1795746223.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.0503\n",
      "Validation Accuracy: 0.5000\n",
      "Validation F1 Score: 0.3333\n",
      "Validation Accuracy on initial 100-sample dataset: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/1795746223.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.9931\n",
      "Validation Accuracy: 0.5000\n",
      "Validation F1 Score: 0.3333\n",
      "Validation Accuracy on initial 100-sample dataset: 0.5000\n"
     ]
    }
   ],
   "source": [
    "_, test_df_100 = clean_data(df_test)\n",
    "test_dataset_100 = SentimentDataset(test_df_100['cleaned'].to_list(), test_df_100['target'].to_list(), gpt_tokenizer, max_length=max_length)\n",
    "test_dataloader_100 = DataLoader(test_dataset_100, batch_size=batch_size)\n",
    "\n",
    "val_loss, val_accuracy, val_f1, val_preds, val_labels = evaluate_model_on_test_dataset(test_dataloader_100, \"/kaggle/working/augmented_400_decoder\")\n",
    "print(f\"Validation Accuracy on initial 100-sample dataset: {val_accuracy:.4f}\")\n",
    "\n",
    "val_loss, val_accuracy, val_f1, val_preds, val_labels = evaluate_model_on_test_dataset(test_dataloader_100, \"/kaggle/working/augmented_1000_decoder\")\n",
    "print(f\"Validation Accuracy on initial 100-sample dataset: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164614,
     "modelInstanceId": 142037,
     "sourceId": 166931,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
