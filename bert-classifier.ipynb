{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:19:36.980221Z",
     "iopub.status.busy": "2024-11-14T16:19:36.979394Z",
     "iopub.status.idle": "2024-11-14T16:19:37.540788Z",
     "shell.execute_reply": "2024-11-14T16:19:37.539783Z",
     "shell.execute_reply.started": "2024-11-14T16:19:36.980169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100-sample dataset: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer, DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "seed = 42\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "df_200 = df.sample(n=200, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "df_100 = df_200.iloc[:100].reset_index(drop=True)\n",
    "df_test = df_200.iloc[100:].reset_index(drop=True)\n",
    "print(\"100-sample dataset:\", df_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:19:37.542833Z",
     "iopub.status.busy": "2024-11-14T16:19:37.542469Z",
     "iopub.status.idle": "2024-11-14T16:19:37.593923Z",
     "shell.execute_reply": "2024-11-14T16:19:37.592997Z",
     "shell.execute_reply.started": "2024-11-14T16:19:37.542800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data_df):\n",
    "    data_df['text_length'] = data_df['review'].apply(len)\n",
    "    data_df['target'] = data_df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "    all_words = [word for review in data_df[\"review\"] for word in review.split(\" \")]\n",
    "    vocab = Counter(all_words)\n",
    "    counts = list(vocab.values())\n",
    "    data_df[\"word_count\"] = data_df[\"review\"].apply(lambda x: len(x.split(\" \")))\n",
    "    data_df[\"word_count\"].describe()\n",
    "    data_df[['text_length', 'target']].groupby('target').mean()['text_length']\n",
    "    data_df[data_df[\"target\"] == 0][\"review\"].values[1]\n",
    "    def remove_hashtag(text):\n",
    "        pattern= \"#[\\w\\d]+\"\n",
    "        return re.sub(pattern, \"\", text)\n",
    "\n",
    "    def remove_url(text):\n",
    "        pattern = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "        return re.sub(pattern, \"\", text)\n",
    "        \n",
    "    data_df['text_without_hastag'] = data_df['review'].apply(remove_hashtag)\n",
    "    data_df['text_without_url'] = data_df['text_without_hastag'].apply(remove_url)\n",
    "    pattern = r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "\n",
    "    def count_non_english(text):\n",
    "        return len(re.findall(pattern, text))\n",
    "\n",
    "    data_df['non_english_count'] = data_df['text_without_url'].apply(count_non_english)\n",
    "    def remove_non_english(text):\n",
    "        pattern = r'[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "        return re.sub(pattern, \"\", text)\n",
    "    data_df['text_without_non_english'] = data_df['text_without_url'].apply(remove_non_english)\n",
    "    def remove_markdown(text):\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text) \n",
    "        text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "    data_df['cleaned'] = data_df['text_without_non_english'].apply(remove_markdown)\n",
    "\n",
    "    train_df, val_df= train_test_split(data_df[['cleaned', 'target']], test_size=0.2, random_state=90,shuffle=True, stratify=data_df['target'])\n",
    "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df_100, val_df_100 = clean_data(df_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:19:37.595544Z",
     "iopub.status.busy": "2024-11-14T16:19:37.595204Z",
     "iopub.status.idle": "2024-11-14T16:19:37.760133Z",
     "shell.execute_reply": "2024-11-14T16:19:37.759189Z",
     "shell.execute_reply.started": "2024-11-14T16:19:37.595501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Load the Dataset and Tokenize\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length=128):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_length = 256\n",
    "train_dataset_100 = SentimentDataset(train_df_100['cleaned'].to_list(), train_df_100['target'].to_list(), tokenizer, max_length=max_length)\n",
    "val_dataset_100 = SentimentDataset(val_df_100['cleaned'].to_list(), val_df_100['target'].to_list(), tokenizer, max_length=max_length)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader_100 = DataLoader(train_dataset_100, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_100 = DataLoader(val_dataset_100, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:19:37.763535Z",
     "iopub.status.busy": "2024-11-14T16:19:37.762951Z",
     "iopub.status.idle": "2024-11-14T16:19:37.785998Z",
     "shell.execute_reply": "2024-11-14T16:19:37.785187Z",
     "shell.execute_reply.started": "2024-11-14T16:19:37.763490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, val_dataloader, num_epochs=50, patience=5, clip_value=1.0, learning_rate=2e-5, save_name=\"classifier.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2, \n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Tracking metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect predictions for accuracy calculation\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            train_f1 = f1_score(preds, labels, average=\"weighted\")\n",
    "            all_train_preds.extend(preds)\n",
    "            all_train_labels.extend(labels)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "                val_f1 = f1_score(preds, labels, average=\"weighted\")\n",
    "                all_val_preds.extend(preds)\n",
    "                all_val_labels.extend(labels)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.3f}, Train Accuracy: {train_accuracy:.3f}, Train F1: {train_f1:.3f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.3f}, Val Accuracy: {val_accuracy:.3f}, Val F1: {val_f1:.3f}\")\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), save_name)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Training complete\n",
    "    print(\"Training complete.\")\n",
    "    print(f\"Average epoch time: {sum(epoch_times) / len(epoch_times):.2f} seconds\")\n",
    "    print(f\"Total training time: {sum(epoch_times):.2f} seconds\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:19:37.787612Z",
     "iopub.status.busy": "2024-11-14T16:19:37.787319Z",
     "iopub.status.idle": "2024-11-14T16:20:26.016454Z",
     "shell.execute_reply": "2024-11-14T16:20:26.015545Z",
     "shell.execute_reply.started": "2024-11-14T16:19:37.787581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Time: 4.70s\n",
      "Train Loss: 0.698, Train Accuracy: 0.500, Train F1: 0.431\n",
      "Val Loss: 0.708, Val Accuracy: 0.400, Val F1: 0.406\n",
      "Epoch 2/50 - Time: 4.68s\n",
      "Train Loss: 0.597, Train Accuracy: 0.750, Train F1: 0.684\n",
      "Val Loss: 0.697, Val Accuracy: 0.600, Val F1: 0.617\n",
      "Epoch 3/50 - Time: 4.73s\n",
      "Train Loss: 0.530, Train Accuracy: 0.762, Train F1: 0.754\n",
      "Val Loss: 0.684, Val Accuracy: 0.650, Val F1: 0.658\n",
      "Epoch 4/50 - Time: 4.70s\n",
      "Train Loss: 0.427, Train Accuracy: 0.900, Train F1: 0.868\n",
      "Val Loss: 0.679, Val Accuracy: 0.650, Val F1: 0.651\n",
      "Epoch 5/50 - Time: 4.72s\n",
      "Train Loss: 0.345, Train Accuracy: 0.988, Train F1: 1.000\n",
      "Val Loss: 0.774, Val Accuracy: 0.600, Val F1: 0.617\n",
      "Epoch 6/50 - Time: 4.68s\n",
      "Train Loss: 0.279, Train Accuracy: 0.950, Train F1: 1.000\n",
      "Val Loss: 0.773, Val Accuracy: 0.600, Val F1: 0.617\n",
      "Epoch 7/50 - Time: 4.70s\n",
      "Train Loss: 0.224, Train Accuracy: 1.000, Train F1: 1.000\n",
      "Val Loss: 0.766, Val Accuracy: 0.450, Val F1: 0.463\n",
      "Epoch 8/50 - Time: 4.69s\n",
      "Train Loss: 0.215, Train Accuracy: 0.988, Train F1: 1.000\n",
      "Val Loss: 0.767, Val Accuracy: 0.500, Val F1: 0.505\n",
      "Epoch 9/50 - Time: 4.70s\n",
      "Train Loss: 0.180, Train Accuracy: 1.000, Train F1: 1.000\n",
      "Val Loss: 0.766, Val Accuracy: 0.600, Val F1: 0.604\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Average epoch time: 4.70 seconds\n",
      "Total training time: 42.29 seconds\n",
      "[0.6977112293243408, 0.5967167615890503, 0.5299703975518545, 0.4267732501029968, 0.34525057673454285, 0.27949822942415875, 0.22434193889300028, 0.2145442565282186, 0.17995964487393698]\n",
      "[0.7078556418418884, 0.696905255317688, 0.6843949556350708, 0.6792677640914917, 0.7739065885543823, 0.7726895213127136, 0.765688955783844, 0.7670286893844604, 0.7657952308654785]\n",
      "[0.5, 0.75, 0.7625, 0.9, 0.9875, 0.95, 1.0, 0.9875, 1.0]\n",
      "[0.4, 0.6, 0.65, 0.65, 0.6, 0.6, 0.45, 0.5, 0.6]\n"
     ]
    }
   ],
   "source": [
    "train_results_100 = train_model(train_dataloader_100, val_dataloader_100, save_name=\"encoder_100.pt\")\n",
    "print(train_results_100[\"train_losses\"])\n",
    "print(train_results_100[\"val_losses\"])\n",
    "print(train_results_100[\"train_accuracies\"])\n",
    "print(train_results_100[\"val_accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:20:26.018428Z",
     "iopub.status.busy": "2024-11-14T16:20:26.018095Z",
     "iopub.status.idle": "2024-11-14T16:20:27.181547Z",
     "shell.execute_reply": "2024-11-14T16:20:27.180619Z",
     "shell.execute_reply.started": "2024-11-14T16:20:26.018395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3557818196.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (encoder): Encoder(\n",
       "    (bert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (hidden2mean): Linear(in_features=768, out_features=16, bias=True)\n",
       "    (hidden2logvar): Linear(in_features=768, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=16, out_features=768, bias=True)\n",
       "    (embedding): Embedding(30522, 768)\n",
       "    (gru): GRU(768, 768, batch_first=True)\n",
       "    (output_layer): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, bert_encoder, hidden_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bert = bert_encoder\n",
    "        self.hidden2mean = nn.Linear(hidden_dim, z_dim)\n",
    "        self.hidden2logvar = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]\n",
    "        mean = self.hidden2mean(hidden_state)\n",
    "        logvar = self.hidden2logvar(hidden_state)\n",
    "        return mean, logvar\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(z_dim, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)  # Embedding layer for input tokens\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, z, target_ids=None, teacher_forcing_ratio=0.5):\n",
    "        h = torch.tanh(self.fc(z)).unsqueeze(0)  # Initial hidden state from latent vector z\n",
    "        batch_size = z.size(0)\n",
    "        max_length = target_ids.size(1) if target_ids is not None else 20  # Set max length\n",
    "\n",
    "        # Initialize output tensor to store logits\n",
    "        outputs = torch.zeros(batch_size, max_length, self.output_layer.out_features).to(z.device)\n",
    "        \n",
    "        # Initialize input token (you may replace this with the start token if available)\n",
    "        input_token = torch.zeros(batch_size, 1, hidden_dim).to(z.device)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            output, h = self.gru(input_token, h)\n",
    "            output_logits = self.output_layer(output.squeeze(1))\n",
    "            outputs[:, t, :] = output_logits\n",
    "            \n",
    "            # Teacher forcing: use ground truth with probability teacher_forcing_ratio\n",
    "            if target_ids is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                input_token = self.embedding(target_ids[:, t]).unsqueeze(1)\n",
    "            else:\n",
    "                _, top_token = output_logits.max(dim=1)\n",
    "                input_token = self.embedding(top_token).unsqueeze(1) \n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SentenceVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, z_dim):\n",
    "        super(SentenceVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def sample_z(self, mean, logvar):\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        epsilon = torch.randn_like(std)  # Sample noise\n",
    "        z = mean + std * epsilon  # Sample z\n",
    "        return z\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, target_ids=None, teacher_forcing_ratio=1.0):\n",
    "        # Encode to obtain mean and logvar\n",
    "        mean, logvar = self.encoder(input_ids, attention_mask)\n",
    "        z = self.sample_z(mean, logvar)\n",
    "        # Decode the latent vector z, using target_ids and teacher_forcing_ratio if provided\n",
    "        recon_x = self.decoder(z, target_ids=target_ids, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        return recon_x, mean, logvar\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0):\n",
    "    logits = logits / temperature \n",
    "    probabilities = torch.softmax(logits, dim=-1) \n",
    "    return torch.multinomial(probabilities, 1).squeeze(-1)\n",
    "\n",
    "def load_state_dict(model, filepath):\n",
    "    state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"module.\", \"\") if key.startswith(\"module.\") else key\n",
    "        new_state_dict[new_key] = value\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "hidden_dim = 768\n",
    "z_dim = 16  # Latent space dimensionality\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "encoder = Encoder(distilbert_encoder, hidden_dim, z_dim)\n",
    "decoder = Decoder(z_dim, hidden_dim, vocab_size)\n",
    "model = SentenceVAE(encoder, decoder, z_dim=16)  # Adjust based on your model's structure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "load_state_dict(model, \"/kaggle/input/best-sen/pytorch/default/1/best_sentence_model (3).pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:20:27.183301Z",
     "iopub.status.busy": "2024-11-14T16:20:27.182806Z",
     "iopub.status.idle": "2024-11-14T16:20:27.198951Z",
     "shell.execute_reply": "2024-11-14T16:20:27.197965Z",
     "shell.execute_reply.started": "2024-11-14T16:20:27.183258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_variations(model, input_text, max_length=256, temperature=0.7, top_k=50, num_variations=4, perturb_scale=0.1):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, logvar = model.encoder(input_ids, attention_mask)\n",
    "        \n",
    "        variations = []\n",
    "        for _ in range(num_variations):\n",
    "            noise = torch.randn_like(mean) * perturb_scale\n",
    "            z = model.sample_z(mean, logvar) + noise\n",
    "            \n",
    "            generated_ids = [tokenizer.cls_token_id]\n",
    "            input_token = model.decoder.embedding(torch.tensor([[tokenizer.cls_token_id]]).to(device))\n",
    "            h = torch.tanh(model.decoder.fc(z)).unsqueeze(0)\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                output, h = model.decoder.gru(input_token, h)\n",
    "                logits = model.decoder.output_layer(output.squeeze(1)) / temperature\n",
    "\n",
    "                k = min(top_k, logits.size(-1))\n",
    "                top_k_values, top_k_indices = torch.topk(logits, k)\n",
    "                probabilities = F.softmax(top_k_values, dim=-1)\n",
    "                \n",
    "                next_token_index = torch.multinomial(probabilities, 1).item()\n",
    "                next_token_id = top_k_indices[0, next_token_index].item()\n",
    "\n",
    "                generated_ids.append(next_token_id)\n",
    "                if next_token_id == tokenizer.sep_token_id:\n",
    "                    break\n",
    "\n",
    "                input_token = model.decoder.embedding(torch.tensor([[next_token_id]]).to(device))\n",
    "\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            variations.append(generated_text)\n",
    "        \n",
    "    return variations\n",
    "\n",
    "# Dataset augmentation function\n",
    "def augment_dataset(df, num_variations=4, max_length=256, temperature=0.8, top_k=50, perturb_scale=0.1):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = row['review']\n",
    "        label = row['sentiment']\n",
    "\n",
    "        variations = generate_variations(\n",
    "            model, input_text, max_length=max_length,\n",
    "            temperature=temperature, top_k=top_k, \n",
    "            num_variations=num_variations, perturb_scale=perturb_scale\n",
    "        )\n",
    "\n",
    "        augmented_texts.extend(variations)\n",
    "        augmented_labels.extend([label] * num_variations)\n",
    "\n",
    "    augmented_df = pd.DataFrame({'review': augmented_texts, 'sentiment': augmented_labels})\n",
    "    combined_df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:20:27.200489Z",
     "iopub.status.busy": "2024-11-14T16:20:27.200169Z",
     "iopub.status.idle": "2024-11-14T16:20:41.642570Z",
     "shell.execute_reply": "2024-11-14T16:20:41.641507Z",
     "shell.execute_reply.started": "2024-11-14T16:20:27.200444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmented_df_400 = augment_dataset(df_100, num_variations=1)\n",
    "augmented_df_1000 = augment_dataset(df_100, num_variations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:20:41.644389Z",
     "iopub.status.busy": "2024-11-14T16:20:41.644069Z",
     "iopub.status.idle": "2024-11-14T16:20:41.737882Z",
     "shell.execute_reply": "2024-11-14T16:20:41.737166Z",
     "shell.execute_reply.started": "2024-11-14T16:20:41.644358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmented_train_df_400, augmented_val_df_400 = clean_data(augmented_df_400)\n",
    "augmented_train_df_1000, augmented_val_df_1000 = clean_data(augmented_df_1000)\n",
    "\n",
    "max_length = 256\n",
    "augmented_train_dataset_400 = SentimentDataset(augmented_train_df_400['cleaned'], augmented_train_df_400['target'], tokenizer)\n",
    "augmented_val_dataset_400 = SentimentDataset(augmented_val_df_400['cleaned'], augmented_val_df_400['target'], tokenizer)\n",
    "augmented_train_dataset_1000 = SentimentDataset(augmented_train_df_1000['cleaned'], augmented_train_df_1000['target'], tokenizer)\n",
    "augmented_val_dataset_1000 = SentimentDataset(augmented_val_df_1000['cleaned'], augmented_val_df_1000['target'], tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "augmented_train_dataloader_400 = DataLoader(augmented_train_dataset_400, batch_size=batch_size)\n",
    "augmented_val_dataloader_400 = DataLoader(augmented_val_dataset_400, batch_size=batch_size)\n",
    "augmented_train_dataloader_1000 = DataLoader(augmented_train_dataset_1000, batch_size=batch_size)\n",
    "augmented_val_dataloader_1000 = DataLoader(augmented_val_dataset_1000, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:20:41.740806Z",
     "iopub.status.busy": "2024-11-14T16:20:41.740504Z",
     "iopub.status.idle": "2024-11-14T16:21:33.869145Z",
     "shell.execute_reply": "2024-11-14T16:21:33.868161Z",
     "shell.execute_reply.started": "2024-11-14T16:20:41.740774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Time: 4.39s\n",
      "Train Loss: 0.739, Train Accuracy: 0.444, Train F1: 0.564\n",
      "Val Loss: 0.703, Val Accuracy: 0.525, Val F1: 0.385\n",
      "Epoch 2/50 - Time: 4.39s\n",
      "Train Loss: 0.660, Train Accuracy: 0.606, Train F1: 0.614\n",
      "Val Loss: 0.701, Val Accuracy: 0.475, Val F1: 0.477\n",
      "Epoch 3/50 - Time: 4.44s\n",
      "Train Loss: 0.620, Train Accuracy: 0.669, Train F1: 0.699\n",
      "Val Loss: 0.691, Val Accuracy: 0.625, Val F1: 0.750\n",
      "Epoch 4/50 - Time: 4.46s\n",
      "Train Loss: 0.533, Train Accuracy: 0.863, Train F1: 0.907\n",
      "Val Loss: 0.687, Val Accuracy: 0.625, Val F1: 0.767\n",
      "Epoch 5/50 - Time: 4.37s\n",
      "Train Loss: 0.435, Train Accuracy: 0.938, Train F1: 0.877\n",
      "Val Loss: 0.678, Val Accuracy: 0.625, Val F1: 0.767\n",
      "Epoch 6/50 - Time: 4.45s\n",
      "Train Loss: 0.356, Train Accuracy: 0.944, Train F1: 0.938\n",
      "Val Loss: 0.706, Val Accuracy: 0.550, Val F1: 0.767\n",
      "Epoch 7/50 - Time: 4.41s\n",
      "Train Loss: 0.253, Train Accuracy: 0.981, Train F1: 0.969\n",
      "Val Loss: 0.715, Val Accuracy: 0.550, Val F1: 0.767\n",
      "Epoch 8/50 - Time: 4.43s\n",
      "Train Loss: 0.188, Train Accuracy: 0.994, Train F1: 0.969\n",
      "Val Loss: 0.754, Val Accuracy: 0.550, Val F1: 0.767\n",
      "Epoch 9/50 - Time: 4.46s\n",
      "Train Loss: 0.146, Train Accuracy: 0.994, Train F1: 0.969\n",
      "Val Loss: 0.801, Val Accuracy: 0.550, Val F1: 0.767\n",
      "Epoch 10/50 - Time: 4.48s\n",
      "Train Loss: 0.125, Train Accuracy: 1.000, Train F1: 1.000\n",
      "Val Loss: 0.826, Val Accuracy: 0.500, Val F1: 0.767\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Average epoch time: 4.43 seconds\n",
      "Total training time: 44.28 seconds\n",
      "[0.7393715262413025, 0.659718644618988, 0.619994604587555, 0.5328367352485657, 0.4354721426963806, 0.3556010007858276, 0.2528286427259445, 0.18796464502811433, 0.14596990644931793, 0.12453114539384842]\n",
      "[0.7030225992202759, 0.7007474899291992, 0.6910946071147919, 0.6869914531707764, 0.6782786548137665, 0.7062080502510071, 0.7153168022632599, 0.754336804151535, 0.8005532324314117, 0.8256797194480896]\n",
      "[0.44375, 0.60625, 0.66875, 0.8625, 0.9375, 0.94375, 0.98125, 0.99375, 0.99375, 1.0]\n",
      "[0.525, 0.475, 0.625, 0.625, 0.625, 0.55, 0.55, 0.55, 0.55, 0.5]\n"
     ]
    }
   ],
   "source": [
    "augmented_train_results_400 = train_model(augmented_train_dataloader_400, augmented_val_dataloader_400, save_name=\"augmented_400_encoder\")\n",
    "print(augmented_train_results_400[\"train_losses\"])\n",
    "print(augmented_train_results_400[\"val_losses\"])\n",
    "print(augmented_train_results_400[\"train_accuracies\"])\n",
    "print(augmented_train_results_400[\"val_accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:21:33.870519Z",
     "iopub.status.busy": "2024-11-14T16:21:33.870224Z",
     "iopub.status.idle": "2024-11-14T16:22:33.624430Z",
     "shell.execute_reply": "2024-11-14T16:22:33.623507Z",
     "shell.execute_reply.started": "2024-11-14T16:21:33.870489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Time: 9.86s\n",
      "Train Loss: 0.710, Train Accuracy: 0.502, Train F1: 0.625\n",
      "Val Loss: 0.707, Val Accuracy: 0.520, Val F1: 0.000\n",
      "Epoch 2/50 - Time: 9.81s\n",
      "Train Loss: 0.686, Train Accuracy: 0.540, Train F1: 0.571\n",
      "Val Loss: 0.735, Val Accuracy: 0.540, Val F1: 0.000\n",
      "Epoch 3/50 - Time: 9.73s\n",
      "Train Loss: 0.648, Train Accuracy: 0.605, Train F1: 0.600\n",
      "Val Loss: 0.827, Val Accuracy: 0.520, Val F1: 0.000\n",
      "Epoch 4/50 - Time: 9.69s\n",
      "Train Loss: 0.590, Train Accuracy: 0.705, Train F1: 0.937\n",
      "Val Loss: 0.951, Val Accuracy: 0.480, Val F1: 0.000\n",
      "Epoch 5/50 - Time: 9.71s\n",
      "Train Loss: 0.540, Train Accuracy: 0.748, Train F1: 0.882\n",
      "Val Loss: 0.756, Val Accuracy: 0.520, Val F1: 0.100\n",
      "Epoch 6/50 - Time: 9.72s\n",
      "Train Loss: 0.426, Train Accuracy: 0.860, Train F1: 1.000\n",
      "Val Loss: 0.766, Val Accuracy: 0.560, Val F1: 0.333\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Average epoch time: 9.75 seconds\n",
      "Total training time: 58.51 seconds\n",
      "[0.7104106958095844, 0.6863334133074834, 0.6479951326663678, 0.5898326016389407, 0.5397789088579324, 0.4261816923434918]\n",
      "[0.7065664380788803, 0.7351633310317993, 0.8271414190530777, 0.9506971538066864, 0.7556823939085007, 0.7662292867898941]\n",
      "[0.5025, 0.54, 0.605, 0.705, 0.7475, 0.86]\n",
      "[0.52, 0.54, 0.52, 0.48, 0.52, 0.56]\n"
     ]
    }
   ],
   "source": [
    "augmented_train_results_1000 = train_model(augmented_train_dataloader_1000, augmented_val_dataloader_1000, save_name=\"augmented_1000_encoder\")\n",
    "print(augmented_train_results_1000[\"train_losses\"])\n",
    "print(augmented_train_results_1000[\"val_losses\"])\n",
    "print(augmented_train_results_1000[\"train_accuracies\"])\n",
    "print(augmented_train_results_1000[\"val_accuracies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:22:33.625960Z",
     "iopub.status.busy": "2024-11-14T16:22:33.625644Z",
     "iopub.status.idle": "2024-11-14T16:22:33.636718Z",
     "shell.execute_reply": "2024-11-14T16:22:33.635730Z",
     "shell.execute_reply.started": "2024-11-14T16:22:33.625927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_dataset(dataloader, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Track metrics\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Calculate loss for each batch\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate predictions\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate average loss, accuracy, and F1 score\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    # Output metrics\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T16:22:33.638441Z",
     "iopub.status.busy": "2024-11-14T16:22:33.638042Z",
     "iopub.status.idle": "2024-11-14T16:22:36.509944Z",
     "shell.execute_reply": "2024-11-14T16:22:36.508999Z",
     "shell.execute_reply.started": "2024-11-14T16:22:33.638400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/1747286799.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5492\n",
      "Validation Accuracy: 0.6500\n",
      "Validation F1 Score: 0.6267\n",
      "Validation Accuracy on initial 200-sample dataset: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_30/1747286799.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6641\n",
      "Validation Accuracy: 0.8500\n",
      "Validation F1 Score: 0.8496\n",
      "Validation Accuracy on initial 1000-sample dataset: 0.8500\n"
     ]
    }
   ],
   "source": [
    "_, test_df_100 = clean_data(df_test)\n",
    "test_dataset_100 = SentimentDataset(test_df_100['cleaned'].to_list(), test_df_100['target'].to_list(), tokenizer, max_length=max_length)\n",
    "test_dataloader_100 = DataLoader(test_dataset_100, batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_loss, val_accuracy, val_f1, val_preds, val_labels = evaluate_model_on_test_dataset(test_dataloader_100, \"/kaggle/working/augmented_400_encoder\")\n",
    "print(f\"Validation Accuracy on initial 100-sample dataset: {val_accuracy:.4f}\")\n",
    "\n",
    "val_loss, val_accuracy, val_f1, val_preds, val_labels = evaluate_model_on_test_dataset(test_dataloader_100, \"/kaggle/working/augmented_1000_encoder\")\n",
    "print(f\"Validation Accuracy on initial 1000-sample dataset: {val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5997690,
     "sourceId": 9788478,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6002623,
     "sourceId": 9795120,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6076451,
     "sourceId": 9893476,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 163030,
     "modelInstanceId": 140407,
     "sourceId": 165032,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 163191,
     "modelInstanceId": 140567,
     "sourceId": 165210,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 163192,
     "modelInstanceId": 140568,
     "sourceId": 165212,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 163211,
     "modelInstanceId": 140589,
     "sourceId": 165235,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
